{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "from datasets import list_datasets, load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _setHFToken():\n",
    "    with open(\"../hf_token.txt\", \"r\") as file:\n",
    "        token = file.read()\n",
    "        \n",
    "    return token\n",
    "os.environ['HF_TOKEN'] = _setHFToken()\n",
    "# os.environ['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"jerryjalapeno/nart-100k-synthetic\", split=\"train\", token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib \n",
    "import shutil\n",
    "\n",
    "home_dir = pathlib.Path.home()\n",
    "dataset_dir = home_dir / \".cache\" / \"huggingface\" / \"datasets\"\n",
    "    \n",
    "shutil.rmtree(str(dataset_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'builder_name': 'json',\n",
      " 'citation': '',\n",
      " 'config_name': 'default',\n",
      " 'dataset_name': 'nart-100k-synthetic',\n",
      " 'dataset_size': 416296694,\n",
      " 'description': '',\n",
      " 'download_checksums': {'hf://datasets/jerryjalapeno/nart-100k-synthetic@e10b686e0666b44c4807571ec6d47a230cf62256/vicunaformatfixedfinal.json': {'checksum': None,\n",
      "                                                                                                                                                 'num_bytes': 546475257}},\n",
      " 'download_size': 546475257,\n",
      " 'features': {'conversations': [{'from': Value(dtype='string', id=None),\n",
      "                                 'value': Value(dtype='string', id=None)}],\n",
      "              'id': Value(dtype='string', id=None)},\n",
      " 'homepage': '',\n",
      " 'license': '',\n",
      " 'post_processed': None,\n",
      " 'post_processing_size': None,\n",
      " 'size_in_bytes': 962771951,\n",
      " 'splits': {'train': SplitInfo(name='train',\n",
      "                               num_bytes=416296694,\n",
      "                               num_examples=99086,\n",
      "                               shard_lengths=None,\n",
      "                               dataset_name='nart-100k-synthetic')},\n",
      " 'supervised_keys': None,\n",
      " 'task_templates': None,\n",
      " 'version': 0.0.0}\n"
     ]
    }
   ],
   "source": [
    "pprint(dataset.info.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "I've been feeling so sad and overwhelmed lately. Work has become such a massive source of stress for me.\n"
     ]
    }
   ],
   "source": [
    "temp = dataset[0]\n",
    "temp = temp['conversations'][0]\n",
    "temp = temp['value']\n",
    "print(type(temp))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "i have been feeling so sad and overwhelmed lately. work has become such a massive source of stress for me.\n"
     ]
    }
   ],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "def preprocessText(text):\n",
    "  text = re.sub(r'Alex', '', text)\n",
    "  text = re.sub(r'Charlie', '', text)\n",
    "  text = text.lower().strip()\n",
    "  # remove \", \" when it appears at the start of a sentence\n",
    "  text = re.sub(r'^, ', '', text)\n",
    "  # remove \" .\" with \".\"\n",
    "  text = re.sub(r' \\.', '.', text)\n",
    "  # remove \" ,\" with \",\"\n",
    "  text = re.sub(r' ,', ',', text)\n",
    "  # remove \" ?\" with \"?\"\n",
    "  text = re.sub(r' \\?', '?', text)\n",
    "  # remove \" !\" with \"!\"\n",
    "  text = re.sub(r' \\!', '!', text)\n",
    "  # remove \",.\" with \".\"\n",
    "  text = re.sub(r',\\.', '.', text)\n",
    "  # remove \",?\" with \"?\"\n",
    "  text = re.sub(r',\\?', '?', text)\n",
    "  # remove more than one space\n",
    "  text = re.sub(r' +', ' ', text)\n",
    "\n",
    "  restext = \"\"\n",
    "  for ch in unicodedata.normalize('NFD', text):\n",
    "    if unicodedata.category(ch) != 'Mn':\n",
    "      restext+=ch\n",
    "\n",
    "  # restext = re.sub(r\"([.!?])\", r\" \\1\", restext)\n",
    "  restext = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", restext)\n",
    "\n",
    "  short_forms = {\n",
    "        r\"\\bi ve\\b\": \"i have\",\n",
    "        r\"\\bi m\\b\": \"i am\",\n",
    "        r\"\\bisn t\\b\": \"is not\",\n",
    "        r\"\\baren t\\b\": \"are not\",\n",
    "        r\"\\bwasn t\\b\": \"was not\",\n",
    "        r\"\\bweren t\\b\": \"were not\",\n",
    "        r\"\\bhaven t\\b\": \"have not\",\n",
    "        r\"\\bhasn t\\b\": \"has not\",\n",
    "        r\"\\bhadn t\\b\": \"had not\",\n",
    "        r\"\\bwon t\\b\": \"will not\",\n",
    "        r\"\\bwouldn t\\b\": \"would not\",\n",
    "        r\"\\bdon t\\b\": \"do not\",\n",
    "        r\"\\bdoesn t\\b\": \"does not\",\n",
    "        r\"\\bdidn t\\b\": \"did not\",\n",
    "        r\"\\bcan t\\b\": \"cannot\",\n",
    "        r\"\\bcouldn t\\b\": \"could not\",\n",
    "        r\"\\bshouldn t\\b\": \"should not\",\n",
    "        r\"\\bmightn t\\b\": \"might not\",\n",
    "        r\"\\bmustn t\\b\": \"must not\",\n",
    "        r\"\\bain t\\b\": \"am not\"\n",
    "    }\n",
    "\n",
    "  for short_form, full_form in short_forms.items():\n",
    "      restext = re.sub(short_form, full_form, restext)\n",
    "\n",
    "  return restext.strip()\n",
    "\n",
    "print(preprocessText(temp))\n",
    "\n",
    "def preprocessDataset(row):\n",
    "  id = row['id']\n",
    "  row = row['conversations']\n",
    "  for conversation in row:\n",
    "    if conversation.get('from') == 'human':\n",
    "      conversation['role'] = \"human\"\n",
    "    elif conversation.get('from') == 'gpt':\n",
    "      conversation['role'] = \"assistant\"\n",
    "\n",
    "    conversation['content'] = preprocessText(conversation.get('value'))\n",
    "    del conversation['from']\n",
    "    del conversation['value']\n",
    "\n",
    "  sys_dict = {\n",
    "        'role': \"system\",\n",
    "        'content': SYSTEM_PROMPT\n",
    "    }\n",
    "  row.insert(0, sys_dict)\n",
    "  # Conversational format: messages\n",
    "  return {\"messages\":row}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 99086/99086 [02:38<00:00, 623.39 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset = dataset.map(preprocessDataset, remove_columns=['conversations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<|im_start|>', '<|im_end|>']}\n",
      "{'bos_token': '<bos>', 'eos_token': '<eos>', 'unk_token': '<unk>', 'pad_token': '<pad>', 'additional_special_tokens': ['<start_of_turn>', '<end_of_turn>']}\n"
     ]
    }
   ],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"philschmid/gemma-tokenizer-chatml\")\n",
    "tokenizer.padding_side = 'right' \n",
    "original_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-7b\")\n",
    "\n",
    "# get special tokens\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(original_tokenizer.special_tokens_map)\n",
    "\n",
    "assert len(tokenizer) == len(original_tokenizer), \"tokenizer are not having the same length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 99086/99086 [03:01<00:00, 544.70 examples/s]\n",
      "Filter: 100%|██████████| 99086/99086 [00:07<00:00, 13209.96 examples/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset({\n",
      "    features: ['id', 'messages'],\n",
      "    num_rows: 71878\n",
      "})\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# remove conversation with more than 1024 tokens, for training memory reasons.\n",
    "dataset = dataset.map(lambda x: {\"input_ids_length\": len(tokenizer.apply_chat_template(x[\"messages\"]))})\n",
    "# filter out the samples that are too long\n",
    "max_input_length = 1024\n",
    "dataset = dataset.filter(lambda x: x[\"input_ids_length\"] <= max_input_length)\n",
    "dataset = dataset.remove_columns([\"input_ids_length\"])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Saving the dataset (1/1 shards): 100%|██████████| 71878/71878 [00:00<00:00, 124553.06 examples/s]\n"
     ]
    }
   ],
   "source": [
    "dataset.save_to_disk('../dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "mainenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
