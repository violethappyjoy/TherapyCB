{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/21bce026/Work/venv/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import unicodedata\n",
    "import re\n",
    "import os\n",
    "\n",
    "import datasets\n",
    "from datasets import list_datasets, load_dataset\n",
    "from transformers import AutoTokenizer\n",
    "from pprint import pprint"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting transformers==4.38.2\n",
      "  Using cached transformers-4.38.2-py3-none-any.whl.metadata (130 kB)\n",
      "Requirement already satisfied: filelock in /home/21bce026/Work/venv/lib/python3.11/site-packages (from transformers==4.38.2) (3.13.4)\n",
      "Requirement already satisfied: huggingface-hub<1.0,>=0.19.3 in /home/21bce026/Work/venv/lib/python3.11/site-packages (from transformers==4.38.2) (0.22.2)\n",
      "Requirement already satisfied: numpy>=1.17 in /home/21bce026/Work/venv/lib/python3.11/site-packages (from transformers==4.38.2) (1.26.4)\n",
      "Requirement already satisfied: packaging>=20.0 in /home/21bce026/Work/venv/lib/python3.11/site-packages (from transformers==4.38.2) (24.0)\n",
      "Requirement already satisfied: pyyaml>=5.1 in /home/21bce026/Work/venv/lib/python3.11/site-packages (from transformers==4.38.2) (6.0.1)\n",
      "Requirement already satisfied: regex!=2019.12.17 in /home/21bce026/Work/venv/lib/python3.11/site-packages (from transformers==4.38.2) (2024.4.16)\n",
      "Requirement already satisfied: requests in /home/21bce026/Work/venv/lib/python3.11/site-packages (from transformers==4.38.2) (2.31.0)\n",
      "Collecting tokenizers<0.19,>=0.14 (from transformers==4.38.2)\n",
      "  Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n",
      "Requirement already satisfied: safetensors>=0.4.1 in /home/21bce026/Work/venv/lib/python3.11/site-packages (from transformers==4.38.2) (0.4.3)\n",
      "Requirement already satisfied: tqdm>=4.27 in /home/21bce026/Work/venv/lib/python3.11/site-packages (from transformers==4.38.2) (4.66.2)\n",
      "Requirement already satisfied: fsspec>=2023.5.0 in /home/21bce026/Work/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (2024.3.1)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in /home/21bce026/Work/venv/lib/python3.11/site-packages (from huggingface-hub<1.0,>=0.19.3->transformers==4.38.2) (4.11.0)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/21bce026/Work/venv/lib/python3.11/site-packages (from requests->transformers==4.38.2) (3.3.2)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/21bce026/Work/venv/lib/python3.11/site-packages (from requests->transformers==4.38.2) (3.7)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /home/21bce026/Work/venv/lib/python3.11/site-packages (from requests->transformers==4.38.2) (2.2.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/21bce026/Work/venv/lib/python3.11/site-packages (from requests->transformers==4.38.2) (2024.2.2)\n",
      "Using cached transformers-4.38.2-py3-none-any.whl (8.5 MB)\n",
      "Using cached tokenizers-0.15.2-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (3.6 MB)\n",
      "Installing collected packages: tokenizers, transformers\n",
      "  Attempting uninstall: tokenizers\n",
      "    Found existing installation: tokenizers 0.19.1\n",
      "    Uninstalling tokenizers-0.19.1:\n",
      "      Successfully uninstalled tokenizers-0.19.1\n",
      "\u001b[33m  WARNING: Failed to remove contents in a temporary directory '/home/21bce026/Work/venv/lib/python3.11/site-packages/~~kenizers'.\n",
      "  You can safely remove it manually.\u001b[0m\u001b[33m\n",
      "\u001b[0m  Attempting uninstall: transformers\n",
      "    Found existing installation: transformers 4.40.1\n",
      "    Uninstalling transformers-4.40.1:\n",
      "      Successfully uninstalled transformers-4.40.1\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "adapter-transformers 3.2.1.post0 requires huggingface-hub<0.14.0,>=0.11.0, but you have huggingface-hub 0.22.2 which is incompatible.\n",
      "adapter-transformers 3.2.1.post0 requires tokenizers!=0.11.3,<0.14,>=0.11.1, but you have tokenizers 0.15.2 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed tokenizers-0.15.2 transformers-4.38.2\n"
     ]
    }
   ],
   "source": [
    "!pip install transformers==4.38.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def _setHFToken():\n",
    "    with open(\"../hf_token.txt\", \"r\") as file:\n",
    "        token = file.read()\n",
    "        \n",
    "    return token\n",
    "os.environ['HF_TOKEN'] = _setHFToken()\n",
    "# os.environ['HF_TOKEN']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.listdir('./')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = load_dataset(\"jerryjalapeno/nart-100k-synthetic\", split=\"train\", token=os.environ['HF_TOKEN'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pathlib \n",
    "import shutil\n",
    "\n",
    "home_dir = pathlib.Path.home()\n",
    "dataset_dir = home_dir / \".cache\" / \"huggingface\" / \"datasets\"\n",
    "    \n",
    "shutil.rmtree(str(dataset_dir))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pprint(dataset.info.__dict__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp = dataset[0]\n",
    "temp = temp['conversations'][0]\n",
    "temp = temp['value']\n",
    "print(type(temp))\n",
    "print(temp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "SYSTEM_PROMPT = \"\"\"You are a helpful and joyous mental therapy assistant. Always answer as helpfully and cheerfully as possible, while being safe.  Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content.Please ensure that your responses are socially unbiased and positive in nature.\\n\\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information.\"\"\"\n",
    "def preprocessText(text):\n",
    "  text = re.sub(r'Alex', '', text)\n",
    "  text = re.sub(r'Charlie', '', text)\n",
    "  text = text.lower().strip()\n",
    "  # remove \", \" when it appears at the start of a sentence\n",
    "  text = re.sub(r'^, ', '', text)\n",
    "  # remove \" .\" with \".\"\n",
    "  text = re.sub(r' \\.', '.', text)\n",
    "  # remove \" ,\" with \",\"\n",
    "  text = re.sub(r' ,', ',', text)\n",
    "  # remove \" ?\" with \"?\"\n",
    "  text = re.sub(r' \\?', '?', text)\n",
    "  # remove \" !\" with \"!\"\n",
    "  text = re.sub(r' \\!', '!', text)\n",
    "  # remove \",.\" with \".\"\n",
    "  text = re.sub(r',\\.', '.', text)\n",
    "  # remove \",?\" with \"?\"\n",
    "  text = re.sub(r',\\?', '?', text)\n",
    "  # remove more than one space\n",
    "  text = re.sub(r' +', ' ', text)\n",
    "\n",
    "  restext = \"\"\n",
    "  for ch in unicodedata.normalize('NFD', text):\n",
    "    if unicodedata.category(ch) != 'Mn':\n",
    "      restext+=ch\n",
    "\n",
    "  # restext = re.sub(r\"([.!?])\", r\" \\1\", restext)\n",
    "  restext = re.sub(r\"[^a-zA-Z.!?]+\", r\" \", restext)\n",
    "\n",
    "  short_forms = {\n",
    "        r\"\\bi ve\\b\": \"i have\",\n",
    "        r\"\\bi m\\b\": \"i am\",\n",
    "        r\"\\bisn t\\b\": \"is not\",\n",
    "        r\"\\baren t\\b\": \"are not\",\n",
    "        r\"\\bwasn t\\b\": \"was not\",\n",
    "        r\"\\bweren t\\b\": \"were not\",\n",
    "        r\"\\bhaven t\\b\": \"have not\",\n",
    "        r\"\\bhasn t\\b\": \"has not\",\n",
    "        r\"\\bhadn t\\b\": \"had not\",\n",
    "        r\"\\bwon t\\b\": \"will not\",\n",
    "        r\"\\bwouldn t\\b\": \"would not\",\n",
    "        r\"\\bdon t\\b\": \"do not\",\n",
    "        r\"\\bdoesn t\\b\": \"does not\",\n",
    "        r\"\\bdidn t\\b\": \"did not\",\n",
    "        r\"\\bcan t\\b\": \"cannot\",\n",
    "        r\"\\bcouldn t\\b\": \"could not\",\n",
    "        r\"\\bshouldn t\\b\": \"should not\",\n",
    "        r\"\\bmightn t\\b\": \"might not\",\n",
    "        r\"\\bmustn t\\b\": \"must not\",\n",
    "        r\"\\bain t\\b\": \"am not\"\n",
    "    }\n",
    "\n",
    "  for short_form, full_form in short_forms.items():\n",
    "      restext = re.sub(short_form, full_form, restext)\n",
    "\n",
    "  return restext.strip()\n",
    "\n",
    "\n",
    "def preprocessDataset(row):\n",
    "  id = row['id']\n",
    "  row = row['conversations']\n",
    "  for conversation in row:\n",
    "    if conversation.get('from') == 'human':\n",
    "      conversation['role'] = \"human\"\n",
    "    elif conversation.get('from') == 'gpt':\n",
    "      conversation['role'] = \"assistant\"\n",
    "\n",
    "    conversation['content'] = preprocessText(conversation.get('value'))\n",
    "    del conversation['from']\n",
    "    del conversation['value']\n",
    "\n",
    "  sys_dict = {\n",
    "        'role': \"system\",\n",
    "        'content': SYSTEM_PROMPT\n",
    "    }\n",
    "  row.insert(0, sys_dict)\n",
    "  # Conversational format: messages\n",
    "  return {\"messages\":row}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = dataset.map(preprocessDataset, remove_columns=['conversations'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"philschmid/gemma-tokenizer-chatml\", token=os.environ['HF_TOKEN'])\n",
    "tokenizer.padding_side = 'right' \n",
    "original_tokenizer = AutoTokenizer.from_pretrained(\"google/gemma-2b-it\", token=os.environ['HF_TOKEN'])\n",
    "\n",
    "# get special tokens\n",
    "print(tokenizer.special_tokens_map)\n",
    "print(original_tokenizer.special_tokens_map)\n",
    "\n",
    "assert len(tokenizer) == len(original_tokenizer), \"tokenizer are not having the same length\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove conversation with more than 1024 tokens, for training memory reasons.\n",
    "dataset = dataset.map(lambda x: {\"input_ids_length\": len(tokenizer.apply_chat_template(x[\"messages\"]))})\n",
    "# filter out the samples that are too long\n",
    "max_input_length = 1024\n",
    "dataset = dataset.filter(lambda x: x[\"input_ids_length\"] <= max_input_length)\n",
    "dataset = dataset.remove_columns([\"input_ids_length\"])\n",
    "print(dataset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset.save_to_disk('../dataset')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
